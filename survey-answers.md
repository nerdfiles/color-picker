# Survey Answers for a "Mid-Level Front-end Responsive Developer" Role 

Taken out of order, unstructured and without the questions preserved. Have a laugh.

## Answers

I've worked with BEM and OOCSS. OOCSS is great for re-use and minding separation of structure and inner elements and inner styles. You're doing less nesting (less specificity) and leveraging the cascade, it's an intuitive way to write CSS though you the rather self-descriptive nature of the structure your styles are intended to account for. The more you write styles that are "coupled" to the HTML structure, the more information you can reason about without having to multitask looking at the HTML itself. So there's a bit of autonomy that comes with writing high specificity styles yet the trade-off is that you lose re-usability (such that you have to override styles; e.g. !important soup is a consequence). So OOCSS is great for scaling and collaborating with other devs who want to re-use your styles, so long as the visual grammar of the web product or website is fairly regular and without very many unique design elements. It's best to use OOCSS, too, since it can better align with vendor libraries you might use without coupling too tightly to them and their structures.

Now specificity isn't inherently bad, but specificity that is chaotic or without much rhyme or reason can make scaling and managing code evolution difficult, and introduce code damage/smell. So BEM is great for controlling specificity, not just arbitrarily introducing low specificity; it's good for managing state regardless of the size of the component and its subcomponents if you separate the modifier to a kind of single hyphen utility class or leverage Block__Element--Variation -Modifier; you can still use utility classes with just the hyphen as it uses double hyphen and double underscore; it's ideal for readability and thinking about groups. Others might disagree about readability. 

Between both, the more you make re-usable, the more you can minify and reason about with static code analysis.

When I first learned about frameworks, I read blogposts that more or less indicated that the purpose of the framework is to hit the ground running but eventually get refactored away so to evolve a framework-based solution into a bespoke solution. So with that in mind, it's a matter of the cognitive overheard and learning curve that can make a framework undesirable. I like frameworks like Angular/AngularJS (front end) because they give opinionated stances about how to structure your application so that you can spend less time on boilerplate workflows. The downside is refactoring them or taking them away or migrating becomes more difficult when the core development team introduces breaking changes between upgrades and updates (e.g. which might break template syntax or breaks methods in the API). At the same time, the developer community itself can be confused about the status of a tool, and this can create a lot of noise around the best strategy to implement or find solutions you're interested in because some developers might call React a "framework" (since it has an opinionated template syntax), which would distinguish it from jQuery which has no templating syntax and historically reminds and informs us of what a "library" is in the JavaScript ecosystem.

So frameworks are usually opinionated about templating, structure, and give clear paths to integrating third-party tools like authentication and authorization, HTTP interception, "providers" and "services" by which you can create models and integrate ORM capabilities. Though at the same time, if thinking ReSTfully, many of the problems around deduping with action queues, cache invalidation and UI capabilities can be reasoned about in negotiation with the API server. So in a sense, the API server's shared understanding can reduce the about of heavy lifting you have to do on the front end.

Frameworks these days make it easier to deploy and build for various other platforms. So I like that a bunch, being able to use Ionic and build to iOS and Android while developing in a web browser. And there's less setup that's involved than back when we had to write our entire build process out in GruntJS or GulpJS (though I enjoyed learning how R.js/RequireJS's build configuration and workflow is made up). Along with that, frameworks cover a lot of the development server environment for you, so you don't have to churn cycles fiddling with that. Angular now comes with E2E and Unit Testing, so there's no need to worry about that either, unless you want to implement Cucumber separately, but there again the community is active enough to have already invented that wheel.

As far as the details of actually coding in Angular, I happen to think its learning curve isn't terribly steeper than React's, and React's JSX actually breaks my ctags in my vim, unless I set its filetype as typescript, which is OK. In any event, I think Angular's templating approach is something quite typical to how we've always been putting JS inside of HTML and I like that about it. If thinking of templating as a criterion for frameworkness, React's templating, while seemingly more elegant, kind of confuses me a bit compared to Angular's when it comes to repeating or looping data. It's readable, but its strange to think about. Angular has a more comfortable feel where it seems like you're doing transclusion or server-side includes. At first blush, JSX feels more like spaghetti code than Angular's templating/binding approach. When it comes to communication between components, they both have relatively the same cognitive overhead while Angular is really only syntactically/superficially more complex (has a bit more ceremony if you want to use EventEmitter).

Browser sniffing (or use Modernizr) with progressive enhancement (start off with the lowest common browser or device or resolution and build up from there to the least common). Check caniuse.com and also use @supports. In JS, create wrappers and check relevant properties of the primitives or other API methods as they've been historically introduced via standards bodies. I guess you could also web scrape MDN browser support pages too and integrate that into your build process to get daily updates on what features you could check for, and then write a small library for checking if those API methods are functions, like we used to do with XMLHttpRequest.

Yes, at Waste Management (before 2011) we had to code for low-vision in our CSS, and be conscious of color contrasts in our color palette. That's when I first started writing semantic HTML but also HTML that had to also harmonize with additional markup so to supply additional structure for features like interactive tabs, carousels, and so on. You want to use the assistive tech your users use, and think about Perceivability, Operability, Usability and Robustness. For perceivability it's a matter of thinking about sensual identification modalities the user will undertake in interacting with your web product, whether it's visual or aural. Ergonomics needs to be thought about as well in this context. Operability concerns being able to navigate web elements and component necessarily through the full range of DOM event possibilities, from mouseless key possibilities to mouse and mobile gestures use, if those elements are available and visible in the DOM or viewport. I happen to think in this context, Hypermedia Controls broadens our understanding of the ways in which we can improve navigability and operosity modalities based on IANA Link Relations. The server's understanding can actually consider the kinds of assistive tech we might use and express new UI modalities as links, rather than the front end having to construct these modalities with complicated DOM behaviors and element creation. (Just a thought.) Understandability is about setting users' expectations and speaking or writing in a way that's appropriate to their goals they need to meet or satisfy with the web product. You want to make the UI memorable and learnable, so it's important sometimes to give visual indicators, feedback or workflows that guide the user to understand, but this shouldn't entitle the UI/UX to make overly cryptic UIs with too much complexity or compromise perceivability. The robustness principle is a long-held established one: be liberal in what you accept (e.g. clients, browsers, entry points, curl requests, etc) and conservative in what you send (don't generate data points for export what the user likely won't be able to use, all else being equal, in some other device or program or network). You want your UI and API to be built-to-last (generational â€” 5-10+ years living web apps; this is where you want to think in terms of graceful degradation as a complement to progressive enhancement). We want the web apps we build today to be living and age-well like the HTML webpages from the 90s, as far as we can while of course meeting our deadlines.

SASS is supreme and works well with BEM and utility classes. I like the ability to use programmatic control structures to generate style and the re-use opportunities, like generating utility classes with loops and maps. Mixins are wonderful for creating higher order abstractions to be customizable and isolatable for elements (e.g. in media query contexts you can use mixins with conditional inner logic, includes on properties and extends which make re-use a breeze). SASS offers variables and functions for accomplishing various design tasks like generating patterns and backgrounds, but many of these features are making their way into CSS itself now. The only real downsides for preprocessors is learning curve (like with frameworks) and the potential to create too many unused classes if you use SASS extend on a shared class that has nested classes or CSS selectors in general. So you could increase the size your CSS output if you combine SASS extend and class re-use that has nested selectors.

I've built a few. SPA's are great when you can use something like ui-router in AngularJS to manage your application like a state machine based on URL mappings. The downside is that you have to more rigorously manage the back button and handle state. So in that case an ORM or some kind of explicit model is important for handling render-to-render caching. With traditional techniques, you can depend on HTTP header settings to give you the model data you expect or form data you expect. The double-edged sword is page refreshing (based on interaction with links): you have to manage state more heavily under certain circumstances if there is no refresh, but in the circumstances you do refresh (like in local dev or in a web browser context), you have to persist data. There are plugins and modules in the community to do this, but it's something to be mindful of. So you'll have to not only preserve cookies or store a token locally, you may have to store a JSON value that represents the state of all your models, what would typically be issued by the server in a traditional server-side CMS or MVC context. You don't have to worry about CORS with traditional techniques, and surely enough with SPA, you have to deal with asynchronous handling of requests and making sure you don't block other features in the critical path of your application, or compromise perceived performance. SPA in general gives a more holistic way of thinking about cache-invalidation-based rendering, precaching, preloading, on-demand lazy loading, and the relation of idleness to response as it measures perceived and actual performance. There's a lot you can do with JS and based on <meta> tags to ensure that you're loading everything you need to give the user a continuous enriching experience.

I like Django's URL routing approach which enables regular expressions and the more abstraction available in URL routing the better. I like Express.js' middleware approach, and it's actually an example of a fairly lightweight framework but which has a very active community creating middleware around it (*cough* Passport, MongoDB/Mongoose, Morgan, Moment, pm2, dotenv, tcomb, etc).

Yes, with my mobile apps I've developed. I've had to configure XML files and dutifully ensure I understand the size supports and collaborate with designers to get that right. I've used media queries like in Backpacker college on the Login page and various other pages to select for high resolution graphics, as well as for LoveStamp (both iOS and Android apps). High resolution also isn't incompatible with compression and you can re-use baseline high resolutions for multiple device dimensions in the web browser, but you could incur the cost of serving too much data than what is required by the client. Usually Leaflet Maps providers offer high resolution tiles at the same base URL.
